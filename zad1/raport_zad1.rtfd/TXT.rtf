{\rtf1\ansi\ansicpg1250\cocoartf1504\cocoasubrtf810
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 W swoim programie korzysta\uc0\u322 em z:\
1. Drzew decyzyjnych z biblioteki tree.\
2. Algorytmu kknn z biblioteki kknn z parametrami: kmax = 21, kernel = \'84optimal\'94, distance = 2\
3. Algorytmu naiveBayes z biblioteki e1071\
4. Biblioteki ROCR do obliczenia miar skuteczno\uc0\u347 ci algorytm\'f3w (auc i accuracy)\
\
Do obliczenia b\uc0\u322 \u281 du empirycznego wykorzysta\u322 em n-krotn\u261  kroswalidacj\u281  dla n = 10, 20, 50, 100.\
Najlepszy okaza\uc0\u322  si\u281  algorytm naiveBayes, nast\u281 pnie kNN, a na ostatnim miejscu drzewa decyzyjne.\
W tabelce zosta\uc0\u322 y umieszczone \u347 rednie wyniki (auc, acc) dla n-krotnej kroswalidacji.\
\
\
\
\
\
 \
\
}